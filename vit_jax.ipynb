{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MYot7DJh9kk"
      },
      "source": [
        "See code at https://github.com/google-research/vision_transformer/\n",
        "\n",
        "See papers at\n",
        "\n",
        "- Vision Transformer: https://arxiv.org/abs/2010.11929\n",
        "- MLP-Mixer: https://arxiv.org/abs/2105.01601\n",
        "- How to train your ViT: https://arxiv.org/abs/2106.10270\n",
        "- When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations: https://arxiv.org/abs/2106.01548\n",
        "\n",
        "This Colab allows you to run the [JAX](https://jax.readthedocs.org) implementation of the Vision Transformer.\n",
        "\n",
        "If you just want to load a pre-trained checkpoint from a large repository and\n",
        "directly use it for inference, you probably want to go [this Colab](https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax_augreg.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXhZm0kpPpH6"
      },
      "source": [
        "##### Copyright 2021 Google LLC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfe6jvTCo1yQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "KfmzfvFxPuk7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOVCm4CnP1Do"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyD76dm5JaeW"
      },
      "source": [
        "### Setup\n",
        "\n",
        "Needs to be executed once in every VM.\n",
        "\n",
        "The cell below downloads the code from Github and install necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "zZvI8OXt78sj",
        "outputId": "40f3fdaa-30a7-4f60-f1f6-014394142a70",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1 style=\"color:red\">CHANGES NOT PERSISTED</h1>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown Select whether you would like to store data in your personal drive.\n",
        "#@markdown\n",
        "#@markdown If you select **yes**, you will need to authorize Colab to access\n",
        "#@markdown your personal drive\n",
        "#@markdown\n",
        "#@markdown If you select **no**, then any changes you make will diappear when\n",
        "#@markdown this Colab's VM restarts after some time of inactivity...\n",
        "use_gdrive = 'no'  #@param [\"yes\", \"no\"]\n",
        "\n",
        "if use_gdrive == 'yes':\n",
        "  from google.colab import drive\n",
        "  drive.mount('/gdrive')\n",
        "  root = '/gdrive/My Drive/vision_transformer_colab'\n",
        "  import os\n",
        "  if not os.path.isdir(root):\n",
        "    os.mkdir(root)\n",
        "  os.chdir(root)\n",
        "  print(f'\\nChanged CWD to \"{root}\"')\n",
        "else:\n",
        "  from IPython import display\n",
        "  display.display(display.HTML(\n",
        "      '<h1 style=\"color:red\">CHANGES NOT PERSISTED</h1>'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GeEy6gN71CDa",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00741e00-a832-4c50-b3c0-86c75236cf3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Already up to date.\n"
          ]
        }
      ],
      "source": [
        "# Clone repository and pull latest changes.\n",
        "![ -d deit ] || git clone --depth=1 https://github.com/facebookresearch/deit\n",
        "!cd deit && git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "sCN4d-GQJdU4",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d16a7101-5a4e-4100-92d1-bfd22cea01b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm==0.3.2 in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: torch>=1.0 in /usr/local/lib/python3.10/dist-packages (from timm==0.3.2) (2.5.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm==0.3.2) (0.20.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0->timm==0.3.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.0->timm==0.3.2) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.3.2) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm==0.3.2) (11.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0->timm==0.3.2) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# DeiT is built on top of timm version 0.3.2, so need to install it first\n",
        "!pip install timm==0.3.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "4SXp9ltq0LXU",
        "outputId": "f6277c98-bccc-4678-f38d-e5d7685abd04",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "metadata": {
        "id": "X8KN6b5H0TCU",
        "outputId": "31f96321-f3b3-4a89-fe27-8ed67c242670",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from torchvision import datasets\n",
        "from PIL import Image\n",
        "\n",
        "def prepare_cifar10_dataset(download_dir, dataset_dir):\n",
        "    \"\"\"\n",
        "    下载 CIFAR-10 数据集并按照 ImageFolder 的格式组织数据。\n",
        "\n",
        "    参数:\n",
        "    - download_dir: 下载数据集的临时目录。\n",
        "    - dataset_dir: 组织后的数据集存储目录（应包含 train 和 val 文件夹）。\n",
        "    \"\"\"\n",
        "    # 定义类别名称\n",
        "    classes = [\n",
        "        'airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "        'dog', 'frog', 'horse', 'ship', 'truck'\n",
        "    ]\n",
        "\n",
        "    # 创建目录结构\n",
        "    train_dir = os.path.join(dataset_dir, 'train')\n",
        "    val_dir = os.path.join(dataset_dir, 'val')\n",
        "\n",
        "    for split_dir in [train_dir, val_dir]:\n",
        "        if not os.path.exists(split_dir):\n",
        "            os.makedirs(split_dir)\n",
        "            print(f\"创建目录: {split_dir}\")\n",
        "\n",
        "        for cls in classes:\n",
        "            cls_dir = os.path.join(split_dir, cls)\n",
        "            if not os.path.exists(cls_dir):\n",
        "                os.makedirs(cls_dir)\n",
        "                print(f\"创建类别目录: {cls_dir}\")\n",
        "\n",
        "    # 下载 CIFAR-10 训练集\n",
        "    train_dataset = datasets.CIFAR10(\n",
        "        root=download_dir,\n",
        "        train=True,\n",
        "        download=True\n",
        "    )\n",
        "\n",
        "    # 下载 CIFAR-10 测试集\n",
        "    val_dataset = datasets.CIFAR10(\n",
        "        root=download_dir,\n",
        "        train=False,\n",
        "        download=True\n",
        "    )\n",
        "\n",
        "    # 保存训练集图像\n",
        "    print(\"开始保存训练集图像...\")\n",
        "    for idx, (img, label) in enumerate(train_dataset):\n",
        "        cls_name = classes[label]\n",
        "        cls_dir = os.path.join(train_dir, cls_name)\n",
        "        img_filename = f\"{cls_name}_{idx:05d}.png\"\n",
        "        img_path = os.path.join(cls_dir, img_filename)\n",
        "        img.save(img_path)\n",
        "        if (idx + 1) % 10000 == 0:\n",
        "            print(f\"已保存 {idx + 1} 张训练图像\")\n",
        "\n",
        "    print(\"训练集图像保存完成。\")\n",
        "\n",
        "    # 保存验证集图像\n",
        "    print(\"开始保存验证集图像...\")\n",
        "    for idx, (img, label) in enumerate(val_dataset):\n",
        "        cls_name = classes[label]\n",
        "        cls_dir = os.path.join(val_dir, cls_name)\n",
        "        img_filename = f\"{cls_name}_{idx:05d}.png\"\n",
        "        img_path = os.path.join(cls_dir, img_filename)\n",
        "        img.save(img_path)\n",
        "        if (idx + 1) % 2000 == 0:\n",
        "            print(f\"已保存 {idx + 1} 张验证图像\")\n",
        "\n",
        "    print(\"验证集图像保存完成。\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 定义下载和组织后的数据集路径\n",
        "DOWNLOAD_DIR = \"./deit/tmp\"\n",
        "DATASET_DIR = \"./deit/data\"\n",
        "\n",
        "# 如果组织后的数据集目录已存在并且包含数据，可以选择跳过\n",
        "if not os.path.exists(DATASET_DIR) or not os.listdir(DATASET_DIR):\n",
        "    prepare_cifar10_dataset(DOWNLOAD_DIR, DATASET_DIR)\n",
        "else:\n",
        "    print(f\"数据集目录 '{DATASET_DIR}' 已存在且不为空。跳过下载和组织步骤。\")\n",
        "\n",
        "    # 清理临时下载目录（可选）\n",
        "if os.path.exists(DOWNLOAD_DIR):\n",
        "    shutil.rmtree(DOWNLOAD_DIR)\n",
        "    print(f\"已删除临时下载目录: {DOWNLOAD_DIR}\")\n"
      ],
      "metadata": {
        "id": "Cy4EZk9hzIL1",
        "outputId": "a2e4bb7e-2532-49c1-815b-29a4dc9827bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "数据集目录 './deit/data' 已存在且不为空。跳过下载和组织步骤。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLBTSXuNjK6"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "BcnlJF7FKTfD",
        "vscode": {
          "languageId": "python"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73668ac0-6ea2-4534-effe-34d6037a2046"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n",
            "预训练模型将被下载到: ./deit/pretrained\n",
            "模型文件已存在: ./deit/pretrained/deit_tiny_distilled_patch16_224-b40b3cf7.pth\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "# 安装 wget 模块\n",
        "!pip install wget\n",
        "\n",
        "import wget  # 用于下载文件\n",
        "\n",
        "# 定义预训练模型的URL\n",
        "pretrained_models = {\n",
        "    \"deit_tiny_distilled_patch16_224\": \"https://dl.fbaipublicfiles.com/deit/deit_tiny_distilled_patch16_224-b40b3cf7.pth\",\n",
        "    \"deit_small_distilled_patch16_224\": \"https://dl.fbaipublicfiles.com/deit/deit_small_distilled_patch16_224-649709d9.pth\",\n",
        "    \"deit_base_distilled_patch16_224\": \"https://dl.fbaipublicfiles.com/deit/deit_base_distilled_patch16_224-df68dfff.pth\"\n",
        "}\n",
        "\n",
        "# 选择要下载的模型名称\n",
        "selected_model = \"deit_tiny_distilled_patch16_224\"  # 可以选择 [\"deit_tiny_distilled_patch16_224\", \"deit_small_distilled_patch16_224\", \"deit_base_distilled_patch16_224\"]\n",
        "\n",
        "# 定义预训练模型存储目录\n",
        "pretrained_dir = \"./deit/pretrained\"\n",
        "\n",
        "# 创建目录（如果不存在）\n",
        "os.makedirs(pretrained_dir, exist_ok=True)\n",
        "print(f\"预训练模型将被下载到: {pretrained_dir}\")\n",
        "\n",
        "# 获取下载链接\n",
        "model_url = pretrained_models[selected_model]\n",
        "model_filename = os.path.basename(model_url)\n",
        "model_path = os.path.join(pretrained_dir, model_filename)\n",
        "\n",
        "# 下载模型文件（如果尚未下载）\n",
        "if not os.path.exists(model_path):\n",
        "    print(f\"正在下载 {selected_model} 模型...\")\n",
        "    wget.download(model_url, out=model_path)\n",
        "    print(f\"\\n模型已下载并保存到: {model_path}\")\n",
        "else:\n",
        "    print(f\"模型文件已存在: {model_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show torchvision\n"
      ],
      "metadata": {
        "id": "4TzqTDH-4X7-",
        "outputId": "53126258-ab59-4cf1-a198-6ee9b50d3fb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: torchvision\n",
            "Version: 0.20.1+cu121\n",
            "Summary: image and video datasets and models for torch deep learning\n",
            "Home-page: https://github.com/pytorch/vision\n",
            "Author: PyTorch Core Team\n",
            "Author-email: soumith@pytorch.org\n",
            "License: BSD\n",
            "Location: /usr/local/lib/python3.10/dist-packages\n",
            "Requires: numpy, pillow, torch\n",
            "Required-by: fastai, timm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. 调整训练脚本并运行\n",
        "import sys\n",
        "import torch\n",
        "from pathlib import Path\n",
        "from argparse import Namespace\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "yoKy_hjz02AM"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 添加项目路径到 sys.path 以导入 main.py\n",
        "project_path = '/content/deit'  # Colab 中的路径\n",
        "if project_path not in sys.path:\n",
        "    sys.path.append(project_path)\n",
        "\n"
      ],
      "metadata": {
        "id": "FXGPmuL15pXc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/deit"
      ],
      "metadata": {
        "id": "XKB5J1W48-jz",
        "outputId": "c0b4fc79-5622-44a4-e052-81ff467f9e24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "augment.py\tLICENSE       patchconvnet_models.py  README_deit.md\t      resmlp_models.py\n",
            "cait_models.py\tlosses.py     pretrained\t      README.md\t\t      run_with_submitit.py\n",
            "data\t\tmain.py       __pycache__\t      README_patchconvnet.md  samplers.py\n",
            "datasets.py\tmodels.py     README_3things.md       README_resmlp.md\t      tox.ini\n",
            "engine.py\tmodels_v2.py  README_cait.md\t      README_revenge.md       utils.py\n",
            "hubconf.py\toutputs       README_cosub.md\t      requirements.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from main import main, get_args_parser"
      ],
      "metadata": {
        "id": "pAhLCXa_5t2r"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 获取默认参数解析器\n",
        "parser = get_args_parser()\n",
        "\n",
        "# 创建一个 Namespace 对象并设置必要的参数\n",
        "args = parser.parse_args([])  # 使用默认参数\n",
        "\n",
        "# 修改参数以适应 CIFAR-10\n",
        "args.model = 'deit_tiny_distilled_patch16_224'\n",
        "args.dataset = 'cifar10'  # 对应 CIFAR-10\n",
        "args.data_path = './deit/data'  # 数据集目录在当前工作目录下的 \"deit/dataset\" 文件夹\n",
        "args.epochs = 20  # 训练 20 个 epoch\n",
        "args.batch_size = 512\n",
        "# args.lr = 0.0005  # 可以根据需要调整学习率\n",
        "args.num_classes = 10  # CIFAR-10 有 10 个类别\n",
        "args.output_dir = './deit/outputs/cifar10pre_tiny'  # 输出目录\n",
        "args.eval = False  # 设置为 True 进行评估\n",
        "args.finetune = './deit/pretrained/deit_tiny_distilled_patch16_224-b40b3cf7.pth'  # 设置为下载的预训练模型路径\n",
        "args.resume = ''  # 如果不需要从检查点恢复训练，可以保持为空\n",
        "args.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# 创建输出目录\n",
        "if args.output_dir:\n",
        "    Path(args.output_dir).mkdir(parents=True, exist_ok=True)\n",
        "    print(f\"输出目录已创建: {args.output_dir}\")\n",
        "\n",
        "# 运行训练\n",
        "main(args)"
      ],
      "metadata": {
        "id": "zXh4frNd5Zx0",
        "outputId": "6f2b814d-fc92-4688-88c4-faf680f99a3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "输出目录已创建: ./deit/outputs/cifar10pre_tiny\n",
            "Not using distributed mode\n",
            "Namespace(batch_size=64, epochs=20, bce_loss=False, unscale_lr=False, model='deit_tiny_distilled_patch16_224', input_size=224, drop=0.0, drop_path=0.1, model_ema=True, model_ema_decay=0.99996, model_ema_force_cpu=False, opt='adamw', opt_eps=1e-08, opt_betas=None, clip_grad=None, momentum=0.9, weight_decay=0.05, sched='cosine', lr=0.0005, lr_noise=None, lr_noise_pct=0.67, lr_noise_std=1.0, warmup_lr=1e-06, min_lr=1e-05, decay_epochs=30, warmup_epochs=5, cooldown_epochs=10, patience_epochs=10, decay_rate=0.1, color_jitter=0.3, aa='rand-m9-mstd0.5-inc1', smoothing=0.1, train_interpolation='bicubic', repeated_aug=True, train_mode=True, ThreeAugment=False, src=False, reprob=0.25, remode='pixel', recount=1, resplit=False, mixup=0.8, cutmix=1.0, cutmix_minmax=None, mixup_prob=1.0, mixup_switch_prob=0.5, mixup_mode='batch', teacher_model='regnety_160', teacher_path='', distillation_type='none', distillation_alpha=0.5, distillation_tau=1.0, cosub=False, finetune='./deit/pretrained/deit_tiny_distilled_patch16_224-b40b3cf7.pth', attn_only=False, data_path='./deit/data', data_set='IMNET', inat_category='name', output_dir='./deit/outputs/cifar10pre_tiny', device='cuda', seed=0, resume='', start_epoch=0, eval=False, eval_crop_ratio=0.875, dist_eval=False, num_workers=10, pin_mem=True, distributed=False, world_size=1, dist_url='env://', dataset='cifar10', num_classes=10)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating model: deit_tiny_distilled_patch16_224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/deit/main.py:279: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(args.finetune, map_location='cpu')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of params: 5910800\n",
            "Start training for 20 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/timm/utils/cuda.py:38: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self._scaler = torch.cuda.amp.GradScaler()\n",
            "/content/deit/engine.py:46: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: [0]  [  0/781]  eta: 1:57:35  lr: 0.000001  loss: 8.3028 (8.3028)  time: 9.0338  data: 5.8394  max mem: 1700\n",
            "Epoch: [0]  [ 10/781]  eta: 0:15:46  lr: 0.000001  loss: 8.4700 (8.4557)  time: 1.2270  data: 0.5392  max mem: 1742\n",
            "Epoch: [0]  [ 20/781]  eta: 0:10:11  lr: 0.000001  loss: 8.3913 (8.4065)  time: 0.3918  data: 0.0071  max mem: 1742\n",
            "Epoch: [0]  [ 30/781]  eta: 0:07:50  lr: 0.000001  loss: 8.2964 (8.3060)  time: 0.2965  data: 0.0054  max mem: 1742\n",
            "Epoch: [0]  [ 40/781]  eta: 0:06:37  lr: 0.000001  loss: 7.9127 (8.1950)  time: 0.2553  data: 0.0051  max mem: 1742\n",
            "Epoch: [0]  [ 50/781]  eta: 0:05:56  lr: 0.000001  loss: 7.8588 (8.1053)  time: 0.2716  data: 0.0055  max mem: 1742\n",
            "Epoch: [0]  [ 60/781]  eta: 0:05:46  lr: 0.000001  loss: 7.5483 (7.9822)  time: 0.3658  data: 0.0057  max mem: 1742\n",
            "Epoch: [0]  [ 70/781]  eta: 0:05:21  lr: 0.000001  loss: 7.2250 (7.8812)  time: 0.3606  data: 0.0041  max mem: 1742\n",
            "Epoch: [0]  [ 80/781]  eta: 0:04:59  lr: 0.000001  loss: 7.0707 (7.7771)  time: 0.2657  data: 0.0053  max mem: 1742\n",
            "Epoch: [0]  [ 90/781]  eta: 0:04:41  lr: 0.000001  loss: 6.8463 (7.6523)  time: 0.2494  data: 0.0050  max mem: 1742\n",
            "Epoch: [0]  [100/781]  eta: 0:04:28  lr: 0.000001  loss: 6.4804 (7.5156)  time: 0.2618  data: 0.0036  max mem: 1742\n",
            "Epoch: [0]  [110/781]  eta: 0:04:27  lr: 0.000001  loss: 6.0798 (7.3804)  time: 0.3631  data: 0.0052  max mem: 1742\n",
            "Epoch: [0]  [120/781]  eta: 0:04:15  lr: 0.000001  loss: 5.8504 (7.2491)  time: 0.3461  data: 0.0060  max mem: 1742\n",
            "Epoch: [0]  [130/781]  eta: 0:04:05  lr: 0.000001  loss: 5.6621 (7.1202)  time: 0.2579  data: 0.0059  max mem: 1742\n",
            "Epoch: [0]  [140/781]  eta: 0:03:56  lr: 0.000001  loss: 5.5118 (7.0064)  time: 0.2618  data: 0.0053  max mem: 1742\n",
            "Epoch: [0]  [150/781]  eta: 0:03:49  lr: 0.000001  loss: 5.4540 (6.8975)  time: 0.2686  data: 0.0051  max mem: 1742\n",
            "Epoch: [0]  [160/781]  eta: 0:03:48  lr: 0.000001  loss: 5.3227 (6.7946)  time: 0.3677  data: 0.0044  max mem: 1742\n",
            "Epoch: [0]  [170/781]  eta: 0:03:41  lr: 0.000001  loss: 5.1321 (6.6964)  time: 0.3572  data: 0.0047  max mem: 1742\n",
            "Epoch: [0]  [180/781]  eta: 0:03:34  lr: 0.000001  loss: 5.1123 (6.6107)  time: 0.2597  data: 0.0050  max mem: 1742\n",
            "Epoch: [0]  [190/781]  eta: 0:03:27  lr: 0.000001  loss: 5.0437 (6.5262)  time: 0.2595  data: 0.0041  max mem: 1742\n",
            "Epoch: [0]  [200/781]  eta: 0:03:23  lr: 0.000001  loss: 5.0437 (6.4529)  time: 0.2918  data: 0.0039  max mem: 1742\n",
            "Epoch: [0]  [210/781]  eta: 0:03:21  lr: 0.000001  loss: 5.0080 (6.3798)  time: 0.3626  data: 0.0049  max mem: 1742\n",
            "Epoch: [0]  [220/781]  eta: 0:03:18  lr: 0.000001  loss: 4.9203 (6.3123)  time: 0.3836  data: 0.0086  max mem: 1742\n",
            "Epoch: [0]  [230/781]  eta: 0:03:13  lr: 0.000001  loss: 4.8749 (6.2485)  time: 0.3254  data: 0.0086  max mem: 1742\n",
            "Epoch: [0]  [240/781]  eta: 0:03:07  lr: 0.000001  loss: 4.7682 (6.1822)  time: 0.2725  data: 0.0058  max mem: 1742\n",
            "Epoch: [0]  [250/781]  eta: 0:03:05  lr: 0.000001  loss: 4.5699 (6.1198)  time: 0.3451  data: 0.0056  max mem: 1742\n",
            "Epoch: [0]  [260/781]  eta: 0:03:01  lr: 0.000001  loss: 4.5699 (6.0637)  time: 0.3642  data: 0.0060  max mem: 1742\n",
            "Epoch: [0]  [270/781]  eta: 0:02:56  lr: 0.000001  loss: 4.5684 (6.0078)  time: 0.2866  data: 0.0054  max mem: 1742\n",
            "Epoch: [0]  [280/781]  eta: 0:02:51  lr: 0.000001  loss: 4.4919 (5.9515)  time: 0.2716  data: 0.0043  max mem: 1742\n",
            "Epoch: [0]  [290/781]  eta: 0:02:47  lr: 0.000001  loss: 4.4577 (5.9010)  time: 0.2947  data: 0.0059  max mem: 1742\n",
            "Epoch: [0]  [300/781]  eta: 0:02:45  lr: 0.000001  loss: 4.4621 (5.8542)  time: 0.3615  data: 0.0075  max mem: 1742\n",
            "Epoch: [0]  [310/781]  eta: 0:02:40  lr: 0.000001  loss: 4.4571 (5.8065)  time: 0.3286  data: 0.0057  max mem: 1742\n",
            "Epoch: [0]  [320/781]  eta: 0:02:36  lr: 0.000001  loss: 4.2936 (5.7572)  time: 0.2588  data: 0.0044  max mem: 1742\n",
            "Epoch: [0]  [330/781]  eta: 0:02:31  lr: 0.000001  loss: 4.2630 (5.7126)  time: 0.2754  data: 0.0065  max mem: 1742\n",
            "Epoch: [0]  [340/781]  eta: 0:02:29  lr: 0.000001  loss: 4.1334 (5.6654)  time: 0.3540  data: 0.0072  max mem: 1742\n",
            "Epoch: [0]  [350/781]  eta: 0:02:26  lr: 0.000001  loss: 4.1201 (5.6225)  time: 0.3708  data: 0.0040  max mem: 1742\n",
            "Epoch: [0]  [360/781]  eta: 0:02:21  lr: 0.000001  loss: 4.0417 (5.5768)  time: 0.2964  data: 0.0048  max mem: 1742\n",
            "Epoch: [0]  [370/781]  eta: 0:02:17  lr: 0.000001  loss: 4.0417 (5.5364)  time: 0.2530  data: 0.0063  max mem: 1742\n",
            "Epoch: [0]  [380/781]  eta: 0:02:13  lr: 0.000001  loss: 4.0470 (5.4959)  time: 0.2488  data: 0.0057  max mem: 1742\n",
            "Epoch: [0]  [390/781]  eta: 0:02:10  lr: 0.000001  loss: 3.9937 (5.4578)  time: 0.3296  data: 0.0058  max mem: 1742\n",
            "Epoch: [0]  [400/781]  eta: 0:02:06  lr: 0.000001  loss: 3.9733 (5.4198)  time: 0.3383  data: 0.0043  max mem: 1746\n",
            "Epoch: [0]  [410/781]  eta: 0:02:02  lr: 0.000001  loss: 3.8916 (5.3823)  time: 0.2632  data: 0.0037  max mem: 1746\n",
            "Epoch: [0]  [420/781]  eta: 0:01:58  lr: 0.000001  loss: 3.8916 (5.3475)  time: 0.2530  data: 0.0050  max mem: 1746\n",
            "Epoch: [0]  [430/781]  eta: 0:01:54  lr: 0.000001  loss: 3.8070 (5.3108)  time: 0.2506  data: 0.0054  max mem: 1746\n",
            "Epoch: [0]  [440/781]  eta: 0:01:52  lr: 0.000001  loss: 3.7454 (5.2767)  time: 0.3311  data: 0.0046  max mem: 1746\n",
            "Epoch: [0]  [450/781]  eta: 0:01:48  lr: 0.000001  loss: 3.8148 (5.2446)  time: 0.3479  data: 0.0044  max mem: 1746\n",
            "Epoch: [0]  [460/781]  eta: 0:01:44  lr: 0.000001  loss: 3.7754 (5.2115)  time: 0.2678  data: 0.0043  max mem: 1746\n",
            "Epoch: [0]  [470/781]  eta: 0:01:41  lr: 0.000001  loss: 3.6809 (5.1800)  time: 0.2675  data: 0.0044  max mem: 1746\n",
            "Epoch: [0]  [480/781]  eta: 0:01:37  lr: 0.000001  loss: 3.7629 (5.1512)  time: 0.2840  data: 0.0054  max mem: 1746\n",
            "Epoch: [0]  [490/781]  eta: 0:01:34  lr: 0.000001  loss: 3.7388 (5.1213)  time: 0.3457  data: 0.0056  max mem: 1746\n",
            "Epoch: [0]  [500/781]  eta: 0:01:31  lr: 0.000001  loss: 3.6498 (5.0924)  time: 0.3240  data: 0.0051  max mem: 1746\n",
            "Epoch: [0]  [510/781]  eta: 0:01:27  lr: 0.000001  loss: 3.6205 (5.0637)  time: 0.2489  data: 0.0057  max mem: 1746\n",
            "Epoch: [0]  [520/781]  eta: 0:01:24  lr: 0.000001  loss: 3.6367 (5.0364)  time: 0.2555  data: 0.0062  max mem: 1746\n",
            "Epoch: [0]  [530/781]  eta: 0:01:20  lr: 0.000001  loss: 3.6367 (5.0104)  time: 0.2644  data: 0.0061  max mem: 1746\n",
            "Epoch: [0]  [540/781]  eta: 0:01:17  lr: 0.000001  loss: 3.6225 (4.9847)  time: 0.3465  data: 0.0076  max mem: 1746\n",
            "Epoch: [0]  [550/781]  eta: 0:01:14  lr: 0.000001  loss: 3.5898 (4.9592)  time: 0.3336  data: 0.0061  max mem: 1746\n",
            "Epoch: [0]  [560/781]  eta: 0:01:10  lr: 0.000001  loss: 3.5715 (4.9340)  time: 0.2522  data: 0.0043  max mem: 1746\n",
            "Epoch: [0]  [570/781]  eta: 0:01:07  lr: 0.000001  loss: 3.5619 (4.9097)  time: 0.2636  data: 0.0053  max mem: 1746\n",
            "Epoch: [0]  [580/781]  eta: 0:01:04  lr: 0.000001  loss: 3.5344 (4.8861)  time: 0.2712  data: 0.0078  max mem: 1746\n",
            "Epoch: [0]  [590/781]  eta: 0:01:01  lr: 0.000001  loss: 3.5318 (4.8642)  time: 0.3362  data: 0.0087  max mem: 1746\n",
            "Epoch: [0]  [600/781]  eta: 0:00:57  lr: 0.000001  loss: 3.5032 (4.8407)  time: 0.3259  data: 0.0069  max mem: 1746\n",
            "Epoch: [0]  [610/781]  eta: 0:00:54  lr: 0.000001  loss: 3.4606 (4.8184)  time: 0.2541  data: 0.0067  max mem: 1746\n",
            "Epoch: [0]  [620/781]  eta: 0:00:51  lr: 0.000001  loss: 3.4688 (4.7975)  time: 0.2574  data: 0.0059  max mem: 1746\n",
            "Epoch: [0]  [630/781]  eta: 0:00:47  lr: 0.000001  loss: 3.4667 (4.7763)  time: 0.2728  data: 0.0062  max mem: 1746\n",
            "Epoch: [0]  [640/781]  eta: 0:00:44  lr: 0.000001  loss: 3.4586 (4.7553)  time: 0.3382  data: 0.0084  max mem: 1746\n",
            "Epoch: [0]  [650/781]  eta: 0:00:41  lr: 0.000001  loss: 3.4602 (4.7351)  time: 0.3273  data: 0.0092  max mem: 1746\n",
            "Epoch: [0]  [660/781]  eta: 0:00:38  lr: 0.000001  loss: 3.4673 (4.7160)  time: 0.2523  data: 0.0067  max mem: 1746\n",
            "Epoch: [0]  [670/781]  eta: 0:00:34  lr: 0.000001  loss: 3.4327 (4.6967)  time: 0.2463  data: 0.0069  max mem: 1746\n",
            "Epoch: [0]  [680/781]  eta: 0:00:31  lr: 0.000001  loss: 3.4284 (4.6782)  time: 0.2859  data: 0.0084  max mem: 1746\n",
            "Epoch: [0]  [690/781]  eta: 0:00:28  lr: 0.000001  loss: 3.4329 (4.6599)  time: 0.3537  data: 0.0070  max mem: 1746\n",
            "Epoch: [0]  [700/781]  eta: 0:00:25  lr: 0.000001  loss: 3.4219 (4.6422)  time: 0.3187  data: 0.0062  max mem: 1746\n",
            "Epoch: [0]  [710/781]  eta: 0:00:22  lr: 0.000001  loss: 3.4137 (4.6246)  time: 0.2466  data: 0.0063  max mem: 1746\n",
            "Epoch: [0]  [720/781]  eta: 0:00:19  lr: 0.000001  loss: 3.3830 (4.6072)  time: 0.2544  data: 0.0087  max mem: 1746\n",
            "Epoch: [0]  [730/781]  eta: 0:00:15  lr: 0.000001  loss: 3.3725 (4.5903)  time: 0.2825  data: 0.0094  max mem: 1746\n",
            "Epoch: [0]  [740/781]  eta: 0:00:12  lr: 0.000001  loss: 3.3745 (4.5739)  time: 0.3478  data: 0.0088  max mem: 1746\n",
            "Epoch: [0]  [750/781]  eta: 0:00:09  lr: 0.000001  loss: 3.3802 (4.5580)  time: 0.3257  data: 0.0089  max mem: 1746\n",
            "Epoch: [0]  [760/781]  eta: 0:00:06  lr: 0.000001  loss: 3.3376 (4.5419)  time: 0.2569  data: 0.0085  max mem: 1746\n",
            "Epoch: [0]  [770/781]  eta: 0:00:03  lr: 0.000001  loss: 3.3485 (4.5266)  time: 0.1987  data: 0.0050  max mem: 1746\n",
            "Epoch: [0]  [780/781]  eta: 0:00:00  lr: 0.000001  loss: 3.3699 (4.5119)  time: 0.1282  data: 0.0006  max mem: 1746\n",
            "Epoch: [0] Total time: 0:04:01 (0.3087 s / it)\n",
            "Averaged stats: lr: 0.000001  loss: 3.3699 (4.5119)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/content/deit/engine.py:97: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test:  [  0/105]  eta: 0:12:11  loss: 2.6065 (2.6065)  acc1: 20.8333 (20.8333)  acc5: 63.5417 (63.5417)  time: 6.9624  data: 5.2633  max mem: 1746\n",
            "Test:  [ 10/105]  eta: 0:01:11  loss: 2.7166 (2.6436)  acc1: 19.7917 (19.9811)  acc5: 64.5833 (65.7197)  time: 0.7537  data: 0.5065  max mem: 1746\n",
            "Test:  [ 20/105]  eta: 0:00:42  loss: 1.7157 (2.0969)  acc1: 34.3750 (39.5833)  acc5: 79.1667 (77.7778)  time: 0.1799  data: 0.0696  max mem: 1746\n",
            "Test:  [ 30/105]  eta: 0:00:30  loss: 1.7157 (2.3087)  acc1: 17.7083 (31.3844)  acc5: 68.7500 (70.9341)  time: 0.2114  data: 0.0950  max mem: 1746\n",
            "Test:  [ 40/105]  eta: 0:00:23  loss: 2.8140 (2.4391)  acc1: 5.2083 (24.1870)  acc5: 55.2083 (66.8699)  time: 0.2129  data: 0.0988  max mem: 1746\n",
            "Test:  [ 50/105]  eta: 0:00:19  loss: 2.4065 (2.3397)  acc1: 5.2083 (25.7149)  acc5: 60.4167 (71.3440)  time: 0.2603  data: 0.1419  max mem: 1746\n",
            "Test:  [ 60/105]  eta: 0:00:15  loss: 1.9709 (2.3052)  acc1: 29.1667 (25.6660)  acc5: 84.3750 (72.8654)  time: 0.3070  data: 0.1610  max mem: 1746\n",
            "Test:  [ 70/105]  eta: 0:00:11  loss: 1.9921 (2.2375)  acc1: 30.2083 (27.5968)  acc5: 82.2917 (75.1027)  time: 0.2590  data: 0.1185  max mem: 1746\n",
            "Test:  [ 80/105]  eta: 0:00:07  loss: 1.9042 (2.2268)  acc1: 33.3333 (27.1991)  acc5: 85.4167 (76.0802)  time: 0.2054  data: 0.1018  max mem: 1746\n",
            "Test:  [ 90/105]  eta: 0:00:04  loss: 2.2693 (2.3358)  acc1: 17.7083 (24.7482)  acc5: 76.0417 (70.6387)  time: 0.1920  data: 0.1060  max mem: 1746\n",
            "Test:  [100/105]  eta: 0:00:01  loss: 3.1474 (2.3798)  acc1: 3.1250 (23.7211)  acc5: 27.0833 (69.7195)  time: 0.1198  data: 0.0534  max mem: 1746\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 2.6381 (2.3839)  acc1: 12.5000 (23.6400)  acc5: 72.9167 (70.2100)  time: 0.0725  data: 0.0128  max mem: 1746\n",
            "Test: Total time: 0:00:28 (0.2676 s / it)\n",
            "* Acc@1 23.640 Acc@5 70.210 loss 2.384\n",
            "Accuracy of the network on the 10000 test images: 23.6%\n",
            "Max accuracy: 23.64%\n",
            "Epoch: [1]  [  0/781]  eta: 1:25:34  lr: 0.000001  loss: 3.1895 (3.1895)  time: 6.5744  data: 5.8994  max mem: 1746\n",
            "Epoch: [1]  [ 10/781]  eta: 0:11:38  lr: 0.000001  loss: 3.2598 (3.2860)  time: 0.9059  data: 0.5431  max mem: 1746\n",
            "Epoch: [1]  [ 20/781]  eta: 0:07:38  lr: 0.000001  loss: 3.2925 (3.2945)  time: 0.3040  data: 0.0073  max mem: 1746\n",
            "Epoch: [1]  [ 30/781]  eta: 0:06:05  lr: 0.000001  loss: 3.2783 (3.2989)  time: 0.2571  data: 0.0085  max mem: 1746\n",
            "Epoch: [1]  [ 40/781]  eta: 0:05:31  lr: 0.000001  loss: 3.3417 (3.3208)  time: 0.2835  data: 0.0095  max mem: 1746\n",
            "Epoch: [1]  [ 50/781]  eta: 0:05:15  lr: 0.000001  loss: 3.3171 (3.3149)  time: 0.3455  data: 0.0101  max mem: 1746\n",
            "Epoch: [1]  [ 60/781]  eta: 0:04:50  lr: 0.000001  loss: 3.2866 (3.3081)  time: 0.3136  data: 0.0099  max mem: 1746\n",
            "Epoch: [1]  [ 70/781]  eta: 0:04:31  lr: 0.000001  loss: 3.3215 (3.3111)  time: 0.2550  data: 0.0073  max mem: 1746\n",
            "Epoch: [1]  [ 80/781]  eta: 0:04:17  lr: 0.000001  loss: 3.3314 (3.3104)  time: 0.2603  data: 0.0080  max mem: 1746\n",
            "Epoch: [1]  [ 90/781]  eta: 0:04:10  lr: 0.000001  loss: 3.2915 (3.3090)  time: 0.2937  data: 0.0095  max mem: 1746\n",
            "Epoch: [1]  [100/781]  eta: 0:04:07  lr: 0.000001  loss: 3.2833 (3.3070)  time: 0.3460  data: 0.0115  max mem: 1746\n",
            "Epoch: [1]  [110/781]  eta: 0:03:57  lr: 0.000001  loss: 3.2835 (3.3024)  time: 0.3122  data: 0.0106  max mem: 1746\n",
            "Epoch: [1]  [120/781]  eta: 0:03:48  lr: 0.000001  loss: 3.2655 (3.2988)  time: 0.2556  data: 0.0092  max mem: 1746\n",
            "Epoch: [1]  [130/781]  eta: 0:03:40  lr: 0.000001  loss: 3.2330 (3.2946)  time: 0.2526  data: 0.0106  max mem: 1746\n",
            "Epoch: [1]  [140/781]  eta: 0:03:36  lr: 0.000001  loss: 3.2414 (3.2923)  time: 0.2881  data: 0.0084  max mem: 1746\n",
            "Epoch: [1]  [150/781]  eta: 0:03:34  lr: 0.000001  loss: 3.2716 (3.2904)  time: 0.3520  data: 0.0093  max mem: 1746\n",
            "Epoch: [1]  [160/781]  eta: 0:03:27  lr: 0.000001  loss: 3.2094 (3.2845)  time: 0.3061  data: 0.0097  max mem: 1746\n",
            "Epoch: [1]  [170/781]  eta: 0:03:21  lr: 0.000001  loss: 3.1987 (3.2795)  time: 0.2468  data: 0.0085  max mem: 1746\n",
            "Epoch: [1]  [180/781]  eta: 0:03:15  lr: 0.000001  loss: 3.2164 (3.2757)  time: 0.2562  data: 0.0081  max mem: 1746\n",
            "Epoch: [1]  [190/781]  eta: 0:03:12  lr: 0.000001  loss: 3.2309 (3.2739)  time: 0.2916  data: 0.0082  max mem: 1746\n",
            "Epoch: [1]  [200/781]  eta: 0:03:10  lr: 0.000001  loss: 3.2215 (3.2710)  time: 0.3523  data: 0.0091  max mem: 1746\n",
            "Epoch: [1]  [210/781]  eta: 0:03:05  lr: 0.000001  loss: 3.2194 (3.2694)  time: 0.3170  data: 0.0089  max mem: 1746\n",
            "Epoch: [1]  [220/781]  eta: 0:02:59  lr: 0.000001  loss: 3.2018 (3.2650)  time: 0.2501  data: 0.0080  max mem: 1746\n",
            "Epoch: [1]  [230/781]  eta: 0:02:55  lr: 0.000001  loss: 3.1706 (3.2617)  time: 0.2557  data: 0.0085  max mem: 1746\n",
            "Epoch: [1]  [240/781]  eta: 0:02:51  lr: 0.000001  loss: 3.1813 (3.2583)  time: 0.2772  data: 0.0080  max mem: 1746\n",
            "Epoch: [1]  [250/781]  eta: 0:02:49  lr: 0.000001  loss: 3.1813 (3.2550)  time: 0.3372  data: 0.0088  max mem: 1746\n",
            "Epoch: [1]  [260/781]  eta: 0:02:45  lr: 0.000001  loss: 3.1784 (3.2530)  time: 0.3218  data: 0.0091  max mem: 1746\n",
            "Epoch: [1]  [270/781]  eta: 0:02:41  lr: 0.000001  loss: 3.1784 (3.2515)  time: 0.2553  data: 0.0075  max mem: 1746\n",
            "Epoch: [1]  [280/781]  eta: 0:02:36  lr: 0.000001  loss: 3.1960 (3.2488)  time: 0.2475  data: 0.0072  max mem: 1746\n",
            "Epoch: [1]  [290/781]  eta: 0:02:33  lr: 0.000001  loss: 3.1897 (3.2463)  time: 0.2785  data: 0.0076  max mem: 1746\n",
            "Epoch: [1]  [300/781]  eta: 0:02:31  lr: 0.000001  loss: 3.1701 (3.2448)  time: 0.3439  data: 0.0103  max mem: 1746\n",
            "Epoch: [1]  [310/781]  eta: 0:02:27  lr: 0.000001  loss: 3.1751 (3.2423)  time: 0.3137  data: 0.0111  max mem: 1746\n",
            "Epoch: [1]  [320/781]  eta: 0:02:23  lr: 0.000001  loss: 3.1915 (3.2417)  time: 0.2488  data: 0.0096  max mem: 1746\n",
            "Epoch: [1]  [330/781]  eta: 0:02:19  lr: 0.000001  loss: 3.2088 (3.2403)  time: 0.2489  data: 0.0078  max mem: 1746\n",
            "Epoch: [1]  [340/781]  eta: 0:02:16  lr: 0.000001  loss: 3.1928 (3.2379)  time: 0.2765  data: 0.0078  max mem: 1746\n",
            "Epoch: [1]  [350/781]  eta: 0:02:13  lr: 0.000001  loss: 3.1935 (3.2372)  time: 0.3398  data: 0.0092  max mem: 1746\n",
            "Epoch: [1]  [360/781]  eta: 0:02:10  lr: 0.000001  loss: 3.1967 (3.2358)  time: 0.3155  data: 0.0087  max mem: 1746\n",
            "Epoch: [1]  [370/781]  eta: 0:02:06  lr: 0.000001  loss: 3.1774 (3.2352)  time: 0.2488  data: 0.0064  max mem: 1746\n",
            "Epoch: [1]  [380/781]  eta: 0:02:03  lr: 0.000001  loss: 3.2023 (3.2338)  time: 0.2693  data: 0.0062  max mem: 1746\n",
            "Epoch: [1]  [390/781]  eta: 0:01:59  lr: 0.000001  loss: 3.1522 (3.2312)  time: 0.2950  data: 0.0064  max mem: 1746\n",
            "Epoch: [1]  [400/781]  eta: 0:01:57  lr: 0.000001  loss: 3.1522 (3.2303)  time: 0.3448  data: 0.0074  max mem: 1746\n",
            "Epoch: [1]  [410/781]  eta: 0:01:53  lr: 0.000001  loss: 3.1683 (3.2281)  time: 0.3150  data: 0.0089  max mem: 1746\n",
            "Epoch: [1]  [420/781]  eta: 0:01:50  lr: 0.000001  loss: 3.1449 (3.2265)  time: 0.2474  data: 0.0087  max mem: 1746\n",
            "Epoch: [1]  [430/781]  eta: 0:01:46  lr: 0.000001  loss: 3.1458 (3.2247)  time: 0.2478  data: 0.0086  max mem: 1746\n",
            "Epoch: [1]  [440/781]  eta: 0:01:43  lr: 0.000001  loss: 3.1567 (3.2224)  time: 0.2777  data: 0.0089  max mem: 1746\n",
            "Epoch: [1]  [450/781]  eta: 0:01:41  lr: 0.000001  loss: 3.1273 (3.2201)  time: 0.3475  data: 0.0087  max mem: 1746\n",
            "Epoch: [1]  [460/781]  eta: 0:01:37  lr: 0.000001  loss: 3.1539 (3.2194)  time: 0.3126  data: 0.0083  max mem: 1746\n",
            "Epoch: [1]  [470/781]  eta: 0:01:34  lr: 0.000001  loss: 3.1559 (3.2178)  time: 0.2499  data: 0.0108  max mem: 1746\n",
            "Epoch: [1]  [480/781]  eta: 0:01:31  lr: 0.000001  loss: 3.1407 (3.2162)  time: 0.2515  data: 0.0116  max mem: 1746\n",
            "Epoch: [1]  [490/781]  eta: 0:01:28  lr: 0.000001  loss: 3.1336 (3.2147)  time: 0.2773  data: 0.0085  max mem: 1746\n",
            "Epoch: [1]  [500/781]  eta: 0:01:25  lr: 0.000001  loss: 3.1463 (3.2136)  time: 0.3433  data: 0.0086  max mem: 1746\n",
            "Epoch: [1]  [510/781]  eta: 0:01:22  lr: 0.000001  loss: 3.1463 (3.2118)  time: 0.3127  data: 0.0086  max mem: 1746\n",
            "Epoch: [1]  [520/781]  eta: 0:01:18  lr: 0.000001  loss: 3.1408 (3.2108)  time: 0.2441  data: 0.0073  max mem: 1746\n",
            "Epoch: [1]  [530/781]  eta: 0:01:15  lr: 0.000001  loss: 3.1352 (3.2089)  time: 0.2515  data: 0.0086  max mem: 1746\n",
            "Epoch: [1]  [540/781]  eta: 0:01:12  lr: 0.000001  loss: 3.1058 (3.2065)  time: 0.2744  data: 0.0089  max mem: 1746\n",
            "Epoch: [1]  [550/781]  eta: 0:01:09  lr: 0.000001  loss: 3.1058 (3.2047)  time: 0.3378  data: 0.0084  max mem: 1746\n",
            "Epoch: [1]  [560/781]  eta: 0:01:06  lr: 0.000001  loss: 3.1205 (3.2032)  time: 0.3193  data: 0.0074  max mem: 1746\n",
            "Epoch: [1]  [570/781]  eta: 0:01:03  lr: 0.000001  loss: 3.1102 (3.2017)  time: 0.2494  data: 0.0078  max mem: 1746\n",
            "Epoch: [1]  [580/781]  eta: 0:01:00  lr: 0.000001  loss: 3.1102 (3.2001)  time: 0.2542  data: 0.0088  max mem: 1746\n",
            "Epoch: [1]  [590/781]  eta: 0:00:57  lr: 0.000001  loss: 3.1236 (3.1991)  time: 0.2692  data: 0.0084  max mem: 1746\n",
            "Epoch: [1]  [600/781]  eta: 0:00:54  lr: 0.000001  loss: 3.1024 (3.1974)  time: 0.3396  data: 0.0084  max mem: 1746\n",
            "Epoch: [1]  [610/781]  eta: 0:00:51  lr: 0.000001  loss: 3.0981 (3.1958)  time: 0.3191  data: 0.0091  max mem: 1746\n",
            "Epoch: [1]  [620/781]  eta: 0:00:48  lr: 0.000001  loss: 3.1036 (3.1946)  time: 0.2499  data: 0.0082  max mem: 1746\n",
            "Epoch: [1]  [630/781]  eta: 0:00:45  lr: 0.000001  loss: 3.1135 (3.1936)  time: 0.2502  data: 0.0071  max mem: 1746\n",
            "Epoch: [1]  [640/781]  eta: 0:00:42  lr: 0.000001  loss: 3.1205 (3.1919)  time: 0.2673  data: 0.0079  max mem: 1746\n",
            "Epoch: [1]  [650/781]  eta: 0:00:39  lr: 0.000001  loss: 3.1135 (3.1902)  time: 0.3485  data: 0.0078  max mem: 1746\n",
            "Epoch: [1]  [660/781]  eta: 0:00:36  lr: 0.000001  loss: 3.0826 (3.1886)  time: 0.3203  data: 0.0085  max mem: 1746\n",
            "Epoch: [1]  [670/781]  eta: 0:00:33  lr: 0.000001  loss: 3.0945 (3.1874)  time: 0.2459  data: 0.0088  max mem: 1746\n",
            "Epoch: [1]  [680/781]  eta: 0:00:30  lr: 0.000001  loss: 3.0889 (3.1857)  time: 0.2505  data: 0.0063  max mem: 1746\n",
            "Epoch: [1]  [690/781]  eta: 0:00:27  lr: 0.000001  loss: 3.0681 (3.1844)  time: 0.2788  data: 0.0064  max mem: 1746\n",
            "Epoch: [1]  [700/781]  eta: 0:00:24  lr: 0.000001  loss: 3.0576 (3.1822)  time: 0.3468  data: 0.0091  max mem: 1746\n",
            "Epoch: [1]  [710/781]  eta: 0:00:21  lr: 0.000001  loss: 3.0426 (3.1800)  time: 0.3213  data: 0.0081  max mem: 1746\n",
            "Epoch: [1]  [720/781]  eta: 0:00:18  lr: 0.000001  loss: 3.0525 (3.1786)  time: 0.2502  data: 0.0049  max mem: 1746\n",
            "Epoch: [1]  [730/781]  eta: 0:00:15  lr: 0.000001  loss: 3.0515 (3.1770)  time: 0.2534  data: 0.0050  max mem: 1746\n",
            "Epoch: [1]  [740/781]  eta: 0:00:12  lr: 0.000001  loss: 3.0873 (3.1759)  time: 0.2954  data: 0.0080  max mem: 1746\n",
            "Epoch: [1]  [750/781]  eta: 0:00:09  lr: 0.000001  loss: 3.0873 (3.1744)  time: 0.3442  data: 0.0090  max mem: 1746\n",
            "Epoch: [1]  [760/781]  eta: 0:00:06  lr: 0.000001  loss: 3.0825 (3.1732)  time: 0.3009  data: 0.0076  max mem: 1746\n",
            "Epoch: [1]  [770/781]  eta: 0:00:03  lr: 0.000001  loss: 3.0277 (3.1711)  time: 0.1944  data: 0.0041  max mem: 1746\n",
            "Epoch: [1]  [780/781]  eta: 0:00:00  lr: 0.000001  loss: 3.0201 (3.1695)  time: 0.1340  data: 0.0007  max mem: 1746\n",
            "Epoch: [1] Total time: 0:03:50 (0.2947 s / it)\n",
            "Averaged stats: lr: 0.000001  loss: 3.0201 (3.1695)\n",
            "Test:  [  0/105]  eta: 0:07:46  loss: 1.7334 (1.7334)  acc1: 41.6667 (41.6667)  acc5: 95.8333 (95.8333)  time: 4.4399  data: 4.2210  max mem: 1746\n",
            "Test:  [ 10/105]  eta: 0:01:07  loss: 1.7124 (1.6437)  acc1: 45.8333 (46.8750)  acc5: 91.6667 (91.9508)  time: 0.7080  data: 0.5447  max mem: 1746\n",
            "Test:  [ 20/105]  eta: 0:00:40  loss: 1.2439 (1.3610)  acc1: 57.2917 (57.7877)  acc5: 94.7917 (95.5853)  time: 0.2749  data: 0.1457  max mem: 1746\n",
            "Test:  [ 30/105]  eta: 0:00:29  loss: 1.2439 (1.6606)  acc1: 25.0000 (44.8925)  acc5: 84.3750 (90.1210)  time: 0.2131  data: 0.1091  max mem: 1746\n",
            "Test:  [ 40/105]  eta: 0:00:22  loss: 2.3811 (1.9070)  acc1: 5.2083 (34.4512)  acc5: 73.9583 (84.6799)  time: 0.2127  data: 0.1000  max mem: 1746\n",
            "Test:  [ 50/105]  eta: 0:00:17  loss: 2.2320 (1.8493)  acc1: 5.2083 (36.0294)  acc5: 79.1667 (86.6626)  time: 0.2073  data: 0.0985  max mem: 1746\n",
            "Test:  [ 60/105]  eta: 0:00:13  loss: 1.6254 (1.8534)  acc1: 36.4583 (35.2288)  acc5: 88.5417 (86.4242)  time: 0.2306  data: 0.0965  max mem: 1746\n",
            "Test:  [ 70/105]  eta: 0:00:10  loss: 1.5833 (1.7718)  acc1: 38.5417 (39.3926)  acc5: 88.5417 (87.6027)  time: 0.2998  data: 0.1340  max mem: 1746\n",
            "Test:  [ 80/105]  eta: 0:00:07  loss: 1.2770 (1.7829)  acc1: 54.1667 (38.5159)  acc5: 90.6250 (87.7315)  time: 0.2593  data: 0.1198  max mem: 1746\n",
            "Test:  [ 90/105]  eta: 0:00:04  loss: 2.0467 (1.9112)  acc1: 20.8333 (34.8901)  acc5: 82.2917 (84.1575)  time: 0.1902  data: 0.0885  max mem: 1746\n",
            "Test:  [100/105]  eta: 0:00:01  loss: 2.7788 (1.9177)  acc1: 5.2083 (34.5606)  acc5: 58.3333 (84.2409)  time: 0.1500  data: 0.0784  max mem: 1746\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 1.5811 (1.9012)  acc1: 36.4583 (34.9800)  acc5: 95.8333 (84.6900)  time: 0.0828  data: 0.0223  max mem: 1746\n",
            "Test: Total time: 0:00:27 (0.2615 s / it)\n",
            "* Acc@1 34.980 Acc@5 84.690 loss 1.901\n",
            "Accuracy of the network on the 10000 test images: 35.0%\n",
            "Max accuracy: 34.98%\n",
            "Epoch: [2]  [  0/781]  eta: 1:10:15  lr: 0.000013  loss: 3.0980 (3.0980)  time: 5.3978  data: 4.6194  max mem: 1746\n",
            "Epoch: [2]  [ 10/781]  eta: 0:11:44  lr: 0.000013  loss: 3.0558 (3.0453)  time: 0.9142  data: 0.4283  max mem: 1746\n",
            "Epoch: [2]  [ 20/781]  eta: 0:07:36  lr: 0.000013  loss: 3.0251 (3.0247)  time: 0.3602  data: 0.0084  max mem: 1746\n",
            "Epoch: [2]  [ 30/781]  eta: 0:06:15  lr: 0.000013  loss: 3.0295 (3.0359)  time: 0.2726  data: 0.0085  max mem: 1746\n",
            "Epoch: [2]  [ 40/781]  eta: 0:05:26  lr: 0.000013  loss: 3.0295 (3.0306)  time: 0.2727  data: 0.0111  max mem: 1746\n",
            "Epoch: [2]  [ 50/781]  eta: 0:05:13  lr: 0.000013  loss: 2.9941 (3.0152)  time: 0.3167  data: 0.0114  max mem: 1746\n",
            "Epoch: [2]  [ 60/781]  eta: 0:04:54  lr: 0.000013  loss: 2.9775 (3.0086)  time: 0.3429  data: 0.0079  max mem: 1746\n",
            "Epoch: [2]  [ 70/781]  eta: 0:04:34  lr: 0.000013  loss: 2.9775 (3.0065)  time: 0.2777  data: 0.0081  max mem: 1746\n",
            "Epoch: [2]  [ 80/781]  eta: 0:04:18  lr: 0.000013  loss: 2.9513 (3.0000)  time: 0.2472  data: 0.0086  max mem: 1746\n",
            "Epoch: [2]  [ 90/781]  eta: 0:04:06  lr: 0.000013  loss: 2.9130 (2.9881)  time: 0.2506  data: 0.0070  max mem: 1746\n",
            "Epoch: [2]  [100/781]  eta: 0:04:04  lr: 0.000013  loss: 2.8860 (2.9750)  time: 0.3192  data: 0.0109  max mem: 1746\n",
            "Epoch: [2]  [110/781]  eta: 0:03:57  lr: 0.000013  loss: 2.8865 (2.9678)  time: 0.3457  data: 0.0117  max mem: 1746\n",
            "Epoch: [2]  [120/781]  eta: 0:03:48  lr: 0.000013  loss: 2.8885 (2.9628)  time: 0.2761  data: 0.0083  max mem: 1746\n",
            "Epoch: [2]  [130/781]  eta: 0:03:40  lr: 0.000013  loss: 2.9478 (2.9601)  time: 0.2515  data: 0.0083  max mem: 1746\n",
            "Epoch: [2]  [140/781]  eta: 0:03:33  lr: 0.000013  loss: 2.9478 (2.9589)  time: 0.2601  data: 0.0080  max mem: 1746\n",
            "Epoch: [2]  [150/781]  eta: 0:03:32  lr: 0.000013  loss: 2.8962 (2.9500)  time: 0.3252  data: 0.0102  max mem: 1746\n",
            "Epoch: [2]  [160/781]  eta: 0:03:27  lr: 0.000013  loss: 2.8126 (2.9394)  time: 0.3410  data: 0.0109  max mem: 1746\n",
            "Epoch: [2]  [170/781]  eta: 0:03:21  lr: 0.000013  loss: 2.8382 (2.9337)  time: 0.2783  data: 0.0082  max mem: 1746\n",
            "Epoch: [2]  [180/781]  eta: 0:03:15  lr: 0.000013  loss: 2.8395 (2.9268)  time: 0.2531  data: 0.0071  max mem: 1746\n",
            "Epoch: [2]  [190/781]  eta: 0:03:09  lr: 0.000013  loss: 2.7771 (2.9165)  time: 0.2461  data: 0.0070  max mem: 1746\n",
            "Epoch: [2]  [200/781]  eta: 0:03:09  lr: 0.000013  loss: 2.7771 (2.9126)  time: 0.3297  data: 0.0080  max mem: 1750\n",
            "Epoch: [2]  [210/781]  eta: 0:03:04  lr: 0.000013  loss: 2.8242 (2.9053)  time: 0.3436  data: 0.0079  max mem: 1750\n",
            "Epoch: [2]  [220/781]  eta: 0:02:59  lr: 0.000013  loss: 2.8166 (2.9002)  time: 0.2622  data: 0.0089  max mem: 1750\n",
            "Epoch: [2]  [230/781]  eta: 0:02:55  lr: 0.000013  loss: 2.8203 (2.8959)  time: 0.2577  data: 0.0107  max mem: 1750\n",
            "Epoch: [2]  [240/781]  eta: 0:02:50  lr: 0.000013  loss: 2.8223 (2.8916)  time: 0.2594  data: 0.0095  max mem: 1750\n",
            "Epoch: [2]  [250/781]  eta: 0:02:48  lr: 0.000013  loss: 2.7905 (2.8856)  time: 0.3224  data: 0.0079  max mem: 1750\n",
            "Epoch: [2]  [260/781]  eta: 0:02:45  lr: 0.000013  loss: 2.7703 (2.8825)  time: 0.3398  data: 0.0079  max mem: 1750\n",
            "Epoch: [2]  [270/781]  eta: 0:02:40  lr: 0.000013  loss: 2.8000 (2.8801)  time: 0.2670  data: 0.0078  max mem: 1750\n",
            "Epoch: [2]  [280/781]  eta: 0:02:36  lr: 0.000013  loss: 2.7693 (2.8747)  time: 0.2486  data: 0.0060  max mem: 1750\n",
            "Epoch: [2]  [290/781]  eta: 0:02:32  lr: 0.000013  loss: 2.6968 (2.8690)  time: 0.2536  data: 0.0067  max mem: 1750\n",
            "Epoch: [2]  [300/781]  eta: 0:02:30  lr: 0.000013  loss: 2.6436 (2.8618)  time: 0.3357  data: 0.0077  max mem: 1750\n",
            "Epoch: [2]  [310/781]  eta: 0:02:27  lr: 0.000013  loss: 2.6977 (2.8563)  time: 0.3447  data: 0.0074  max mem: 1750\n",
            "Epoch: [2]  [320/781]  eta: 0:02:23  lr: 0.000013  loss: 2.7819 (2.8535)  time: 0.2702  data: 0.0105  max mem: 1750\n",
            "Epoch: [2]  [330/781]  eta: 0:02:19  lr: 0.000013  loss: 2.7311 (2.8475)  time: 0.2537  data: 0.0097  max mem: 1750\n",
            "Epoch: [2]  [340/781]  eta: 0:02:15  lr: 0.000013  loss: 2.6400 (2.8414)  time: 0.2441  data: 0.0058  max mem: 1750\n",
            "Epoch: [2]  [350/781]  eta: 0:02:13  lr: 0.000013  loss: 2.7318 (2.8381)  time: 0.3440  data: 0.0075  max mem: 1750\n",
            "Epoch: [2]  [360/781]  eta: 0:02:10  lr: 0.000013  loss: 2.6625 (2.8310)  time: 0.3503  data: 0.0101  max mem: 1750\n",
            "Epoch: [2]  [370/781]  eta: 0:02:06  lr: 0.000013  loss: 2.5915 (2.8254)  time: 0.2511  data: 0.0099  max mem: 1750\n",
            "Epoch: [2]  [380/781]  eta: 0:02:02  lr: 0.000013  loss: 2.6274 (2.8212)  time: 0.2462  data: 0.0090  max mem: 1750\n",
            "Epoch: [2]  [390/781]  eta: 0:01:59  lr: 0.000013  loss: 2.6278 (2.8145)  time: 0.2496  data: 0.0076  max mem: 1750\n",
            "Epoch: [2]  [400/781]  eta: 0:01:57  lr: 0.000013  loss: 2.6258 (2.8110)  time: 0.3523  data: 0.0088  max mem: 1750\n",
            "Epoch: [2]  [410/781]  eta: 0:01:53  lr: 0.000013  loss: 2.5626 (2.8047)  time: 0.3496  data: 0.0089  max mem: 1750\n",
            "Epoch: [2]  [420/781]  eta: 0:01:50  lr: 0.000013  loss: 2.5209 (2.8004)  time: 0.2552  data: 0.0078  max mem: 1750\n",
            "Epoch: [2]  [430/781]  eta: 0:01:46  lr: 0.000013  loss: 2.5998 (2.7957)  time: 0.2488  data: 0.0082  max mem: 1750\n",
            "Epoch: [2]  [440/781]  eta: 0:01:43  lr: 0.000013  loss: 2.6501 (2.7923)  time: 0.2559  data: 0.0079  max mem: 1750\n",
            "Epoch: [2]  [450/781]  eta: 0:01:41  lr: 0.000013  loss: 2.7376 (2.7904)  time: 0.3485  data: 0.0060  max mem: 1750\n",
            "Epoch: [2]  [460/781]  eta: 0:01:37  lr: 0.000013  loss: 2.6713 (2.7874)  time: 0.3370  data: 0.0067  max mem: 1750\n",
            "Epoch: [2]  [470/781]  eta: 0:01:34  lr: 0.000013  loss: 2.6967 (2.7858)  time: 0.2553  data: 0.0092  max mem: 1750\n",
            "Epoch: [2]  [480/781]  eta: 0:01:31  lr: 0.000013  loss: 2.7661 (2.7838)  time: 0.2505  data: 0.0087  max mem: 1750\n",
            "Epoch: [2]  [490/781]  eta: 0:01:27  lr: 0.000013  loss: 2.6350 (2.7791)  time: 0.2595  data: 0.0069  max mem: 1750\n",
            "Epoch: [2]  [500/781]  eta: 0:01:25  lr: 0.000013  loss: 2.5202 (2.7729)  time: 0.3399  data: 0.0072  max mem: 1750\n",
            "Epoch: [2]  [510/781]  eta: 0:01:22  lr: 0.000013  loss: 2.6268 (2.7707)  time: 0.3281  data: 0.0091  max mem: 1750\n",
            "Epoch: [2]  [520/781]  eta: 0:01:18  lr: 0.000013  loss: 2.6268 (2.7662)  time: 0.2481  data: 0.0068  max mem: 1750\n",
            "Epoch: [2]  [530/781]  eta: 0:01:15  lr: 0.000013  loss: 2.6195 (2.7644)  time: 0.2494  data: 0.0074  max mem: 1750\n",
            "Epoch: [2]  [540/781]  eta: 0:01:12  lr: 0.000013  loss: 2.6317 (2.7602)  time: 0.2582  data: 0.0087  max mem: 1750\n",
            "Epoch: [2]  [550/781]  eta: 0:01:09  lr: 0.000013  loss: 2.5506 (2.7567)  time: 0.3521  data: 0.0086  max mem: 1750\n",
            "Epoch: [2]  [560/781]  eta: 0:01:06  lr: 0.000013  loss: 2.5761 (2.7537)  time: 0.3380  data: 0.0089  max mem: 1750\n",
            "Epoch: [2]  [570/781]  eta: 0:01:03  lr: 0.000013  loss: 2.6201 (2.7514)  time: 0.2480  data: 0.0090  max mem: 1750\n",
            "Epoch: [2]  [580/781]  eta: 0:01:00  lr: 0.000013  loss: 2.6654 (2.7493)  time: 0.2519  data: 0.0084  max mem: 1750\n",
            "Epoch: [2]  [590/781]  eta: 0:00:57  lr: 0.000013  loss: 2.6417 (2.7465)  time: 0.2552  data: 0.0065  max mem: 1750\n",
            "Epoch: [2]  [600/781]  eta: 0:00:54  lr: 0.000013  loss: 2.6642 (2.7440)  time: 0.3405  data: 0.0079  max mem: 1750\n",
            "Epoch: [2]  [610/781]  eta: 0:00:51  lr: 0.000013  loss: 2.6642 (2.7414)  time: 0.3372  data: 0.0093  max mem: 1750\n",
            "Epoch: [2]  [620/781]  eta: 0:00:48  lr: 0.000013  loss: 2.5663 (2.7382)  time: 0.2504  data: 0.0092  max mem: 1750\n",
            "Epoch: [2]  [630/781]  eta: 0:00:45  lr: 0.000013  loss: 2.6486 (2.7372)  time: 0.2552  data: 0.0082  max mem: 1750\n",
            "Epoch: [2]  [640/781]  eta: 0:00:42  lr: 0.000013  loss: 2.5294 (2.7321)  time: 0.2756  data: 0.0070  max mem: 1750\n",
            "Epoch: [2]  [650/781]  eta: 0:00:39  lr: 0.000013  loss: 2.4291 (2.7296)  time: 0.3477  data: 0.0081  max mem: 1750\n",
            "Epoch: [2]  [660/781]  eta: 0:00:36  lr: 0.000013  loss: 2.4822 (2.7258)  time: 0.3305  data: 0.0083  max mem: 1750\n",
            "Epoch: [2]  [670/781]  eta: 0:00:33  lr: 0.000013  loss: 2.4781 (2.7221)  time: 0.2526  data: 0.0067  max mem: 1750\n",
            "Epoch: [2]  [680/781]  eta: 0:00:30  lr: 0.000013  loss: 2.5104 (2.7200)  time: 0.2470  data: 0.0066  max mem: 1750\n",
            "Epoch: [2]  [690/781]  eta: 0:00:27  lr: 0.000013  loss: 2.5104 (2.7160)  time: 0.2591  data: 0.0064  max mem: 1750\n",
            "Epoch: [2]  [700/781]  eta: 0:00:24  lr: 0.000013  loss: 2.4322 (2.7123)  time: 0.3408  data: 0.0066  max mem: 1750\n",
            "Epoch: [2]  [710/781]  eta: 0:00:21  lr: 0.000013  loss: 2.5613 (2.7115)  time: 0.3309  data: 0.0069  max mem: 1750\n",
            "Epoch: [2]  [720/781]  eta: 0:00:18  lr: 0.000013  loss: 2.6663 (2.7095)  time: 0.2572  data: 0.0084  max mem: 1750\n",
            "Epoch: [2]  [730/781]  eta: 0:00:15  lr: 0.000013  loss: 2.5800 (2.7063)  time: 0.2538  data: 0.0099  max mem: 1750\n",
            "Epoch: [2]  [740/781]  eta: 0:00:12  lr: 0.000013  loss: 2.4805 (2.7036)  time: 0.2710  data: 0.0110  max mem: 1750\n",
            "Epoch: [2]  [750/781]  eta: 0:00:09  lr: 0.000013  loss: 2.5403 (2.7011)  time: 0.3404  data: 0.0100  max mem: 1750\n",
            "Epoch: [2]  [760/781]  eta: 0:00:06  lr: 0.000013  loss: 2.5643 (2.6995)  time: 0.3181  data: 0.0076  max mem: 1750\n",
            "Epoch: [2]  [770/781]  eta: 0:00:03  lr: 0.000013  loss: 2.5750 (2.6976)  time: 0.1932  data: 0.0051  max mem: 1750\n",
            "Epoch: [2]  [780/781]  eta: 0:00:00  lr: 0.000013  loss: 2.6032 (2.6959)  time: 0.1331  data: 0.0014  max mem: 1750\n",
            "Epoch: [2] Total time: 0:03:50 (0.2951 s / it)\n",
            "Averaged stats: lr: 0.000013  loss: 2.6032 (2.6959)\n",
            "Test:  [  0/105]  eta: 0:09:33  loss: 0.4557 (0.4557)  acc1: 88.5417 (88.5417)  acc5: 98.9583 (98.9583)  time: 5.4665  data: 5.2515  max mem: 1750\n",
            "Test:  [ 10/105]  eta: 0:01:08  loss: 0.3777 (0.3563)  acc1: 90.6250 (91.2879)  acc5: 100.0000 (99.8106)  time: 0.7179  data: 0.5238  max mem: 1750\n",
            "Test:  [ 20/105]  eta: 0:00:39  loss: 0.2117 (0.2738)  acc1: 94.7917 (93.7996)  acc5: 100.0000 (99.9008)  time: 0.2205  data: 0.0554  max mem: 1750\n",
            "Test:  [ 30/105]  eta: 0:00:29  loss: 0.3474 (0.4133)  acc1: 86.4583 (89.2137)  acc5: 100.0000 (99.7648)  time: 0.2055  data: 0.0831  max mem: 1750\n",
            "Test:  [ 40/105]  eta: 0:00:22  loss: 0.7829 (0.6810)  acc1: 60.4167 (78.0996)  acc5: 97.9167 (98.8567)  time: 0.2062  data: 0.0960  max mem: 1750\n",
            "Test:  [ 50/105]  eta: 0:00:17  loss: 1.1688 (0.7403)  acc1: 47.9167 (75.6127)  acc5: 98.9583 (99.0196)  time: 0.1909  data: 0.0730  max mem: 1750\n",
            "Test:  [ 60/105]  eta: 0:00:13  loss: 0.7882 (0.7137)  acc1: 70.8333 (77.3736)  acc5: 100.0000 (99.1120)  time: 0.2035  data: 0.0859  max mem: 1750\n",
            "Test:  [ 70/105]  eta: 0:00:10  loss: 0.4631 (0.6685)  acc1: 89.5833 (79.3867)  acc5: 100.0000 (99.1784)  time: 0.2918  data: 0.1518  max mem: 1750\n",
            "Test:  [ 80/105]  eta: 0:00:07  loss: 0.4411 (0.6453)  acc1: 88.5417 (80.2469)  acc5: 98.9583 (99.1255)  time: 0.3123  data: 0.1603  max mem: 1750\n",
            "Test:  [ 90/105]  eta: 0:00:04  loss: 0.5172 (0.6580)  acc1: 84.3750 (80.0252)  acc5: 98.9583 (99.1186)  time: 0.2164  data: 0.0821  max mem: 1750\n",
            "Test:  [100/105]  eta: 0:00:01  loss: 0.6533 (0.6371)  acc1: 78.1250 (80.6931)  acc5: 100.0000 (99.1646)  time: 0.1324  data: 0.0377  max mem: 1750\n",
            "Test:  [104/105]  eta: 0:00:00  loss: 0.3531 (0.6252)  acc1: 89.5833 (81.0400)  acc5: 100.0000 (99.1900)  time: 0.1051  data: 0.0361  max mem: 1750\n",
            "Test: Total time: 0:00:27 (0.2621 s / it)\n",
            "* Acc@1 81.040 Acc@5 99.190 loss 0.625\n",
            "Accuracy of the network on the 10000 test images: 81.0%\n",
            "Max accuracy: 81.04%\n",
            "Epoch: [3]  [  0/781]  eta: 0:51:50  lr: 0.000026  loss: 2.3932 (2.3932)  time: 3.9824  data: 3.4234  max mem: 1750\n",
            "Epoch: [3]  [ 10/781]  eta: 0:11:17  lr: 0.000026  loss: 2.5141 (2.4798)  time: 0.8791  data: 0.3170  max mem: 1750\n",
            "Epoch: [3]  [ 20/781]  eta: 0:07:33  lr: 0.000026  loss: 2.4722 (2.4708)  time: 0.4264  data: 0.0068  max mem: 1750\n",
            "Epoch: [3]  [ 30/781]  eta: 0:06:03  lr: 0.000026  loss: 2.4761 (2.4763)  time: 0.2666  data: 0.0072  max mem: 1750\n",
            "Epoch: [3]  [ 40/781]  eta: 0:05:17  lr: 0.000026  loss: 2.4920 (2.4778)  time: 0.2532  data: 0.0079  max mem: 1750\n",
            "Epoch: [3]  [ 50/781]  eta: 0:04:54  lr: 0.000026  loss: 2.4956 (2.4769)  time: 0.2763  data: 0.0086  max mem: 1750\n",
            "Epoch: [3]  [ 60/781]  eta: 0:04:46  lr: 0.000026  loss: 2.6327 (2.4849)  time: 0.3350  data: 0.0087  max mem: 1750\n",
            "Epoch: [3]  [ 70/781]  eta: 0:04:29  lr: 0.000026  loss: 2.5667 (2.4877)  time: 0.3170  data: 0.0073  max mem: 1750\n",
            "Epoch: [3]  [ 80/781]  eta: 0:04:16  lr: 0.000026  loss: 2.5258 (2.4900)  time: 0.2688  data: 0.0073  max mem: 1750\n",
            "Epoch: [3]  [ 90/781]  eta: 0:04:03  lr: 0.000026  loss: 2.5569 (2.4976)  time: 0.2575  data: 0.0090  max mem: 1750\n",
            "Epoch: [3]  [100/781]  eta: 0:03:58  lr: 0.000026  loss: 2.5569 (2.4976)  time: 0.2866  data: 0.0103  max mem: 1750\n",
            "Epoch: [3]  [110/781]  eta: 0:03:56  lr: 0.000026  loss: 2.5929 (2.5054)  time: 0.3514  data: 0.0100  max mem: 1750\n",
            "Epoch: [3]  [120/781]  eta: 0:03:46  lr: 0.000026  loss: 2.4242 (2.4908)  time: 0.3085  data: 0.0085  max mem: 1750\n",
            "Epoch: [3]  [130/781]  eta: 0:03:39  lr: 0.000026  loss: 2.3976 (2.4883)  time: 0.2517  data: 0.0087  max mem: 1750\n",
            "Epoch: [3]  [140/781]  eta: 0:03:31  lr: 0.000026  loss: 2.5610 (2.4891)  time: 0.2484  data: 0.0075  max mem: 1750\n",
            "Epoch: [3]  [150/781]  eta: 0:03:28  lr: 0.000026  loss: 2.5518 (2.4933)  time: 0.2860  data: 0.0067  max mem: 1750\n",
            "Epoch: [3]  [160/781]  eta: 0:03:25  lr: 0.000026  loss: 2.4310 (2.4847)  time: 0.3425  data: 0.0078  max mem: 1750\n",
            "Epoch: [3]  [170/781]  eta: 0:03:19  lr: 0.000026  loss: 2.3259 (2.4843)  time: 0.3045  data: 0.0099  max mem: 1750\n",
            "Epoch: [3]  [180/781]  eta: 0:03:14  lr: 0.000026  loss: 2.5795 (2.4875)  time: 0.2578  data: 0.0106  max mem: 1750\n",
            "Epoch: [3]  [190/781]  eta: 0:03:08  lr: 0.000026  loss: 2.5802 (2.4882)  time: 0.2485  data: 0.0088  max mem: 1750\n",
            "Epoch: [3]  [200/781]  eta: 0:03:06  lr: 0.000026  loss: 2.5339 (2.4851)  time: 0.2936  data: 0.0083  max mem: 1750\n",
            "Epoch: [3]  [210/781]  eta: 0:03:03  lr: 0.000026  loss: 2.5060 (2.4890)  time: 0.3388  data: 0.0066  max mem: 1750\n",
            "Epoch: [3]  [220/781]  eta: 0:02:58  lr: 0.000026  loss: 2.5796 (2.4938)  time: 0.2908  data: 0.0073  max mem: 1750\n",
            "Epoch: [3]  [230/781]  eta: 0:02:53  lr: 0.000026  loss: 2.5796 (2.4970)  time: 0.2563  data: 0.0099  max mem: 1750\n",
            "Epoch: [3]  [240/781]  eta: 0:02:49  lr: 0.000026  loss: 2.5173 (2.4952)  time: 0.2577  data: 0.0084  max mem: 1750\n",
            "Epoch: [3]  [250/781]  eta: 0:02:46  lr: 0.000026  loss: 2.4756 (2.4912)  time: 0.3058  data: 0.0069  max mem: 1750\n",
            "Epoch: [3]  [260/781]  eta: 0:02:44  lr: 0.000026  loss: 2.5230 (2.4920)  time: 0.3412  data: 0.0106  max mem: 1750\n",
            "Epoch: [3]  [270/781]  eta: 0:02:39  lr: 0.000026  loss: 2.4450 (2.4871)  time: 0.2975  data: 0.0114  max mem: 1750\n",
            "Epoch: [3]  [280/781]  eta: 0:02:35  lr: 0.000026  loss: 2.3990 (2.4856)  time: 0.2523  data: 0.0070  max mem: 1750\n",
            "Epoch: [3]  [290/781]  eta: 0:02:31  lr: 0.000026  loss: 2.5133 (2.4878)  time: 0.2491  data: 0.0065  max mem: 1750\n",
            "Epoch: [3]  [300/781]  eta: 0:02:29  lr: 0.000026  loss: 2.5671 (2.4893)  time: 0.3029  data: 0.0104  max mem: 1750\n",
            "Epoch: [3]  [310/781]  eta: 0:02:26  lr: 0.000026  loss: 2.5385 (2.4844)  time: 0.3366  data: 0.0115  max mem: 1750\n",
            "Epoch: [3]  [320/781]  eta: 0:02:22  lr: 0.000026  loss: 2.5229 (2.4845)  time: 0.2852  data: 0.0086  max mem: 1750\n",
            "Epoch: [3]  [330/781]  eta: 0:02:18  lr: 0.000026  loss: 2.5182 (2.4841)  time: 0.2462  data: 0.0086  max mem: 1750\n",
            "Epoch: [3]  [340/781]  eta: 0:02:14  lr: 0.000026  loss: 2.5284 (2.4838)  time: 0.2469  data: 0.0073  max mem: 1750\n",
            "Epoch: [3]  [350/781]  eta: 0:02:12  lr: 0.000026  loss: 2.5863 (2.4837)  time: 0.3121  data: 0.0051  max mem: 1750\n",
            "Epoch: [3]  [360/781]  eta: 0:02:09  lr: 0.000026  loss: 2.5274 (2.4846)  time: 0.3437  data: 0.0075  max mem: 1750\n",
            "Epoch: [3]  [370/781]  eta: 0:02:05  lr: 0.000026  loss: 2.5183 (2.4852)  time: 0.2935  data: 0.0088  max mem: 1750\n",
            "Epoch: [3]  [380/781]  eta: 0:02:01  lr: 0.000026  loss: 2.4720 (2.4832)  time: 0.2472  data: 0.0071  max mem: 1750\n",
            "Epoch: [3]  [390/781]  eta: 0:01:58  lr: 0.000026  loss: 2.4982 (2.4840)  time: 0.2408  data: 0.0071  max mem: 1750\n",
            "Epoch: [3]  [400/781]  eta: 0:01:55  lr: 0.000026  loss: 2.4911 (2.4789)  time: 0.3136  data: 0.0093  max mem: 1750\n",
            "Epoch: [3]  [410/781]  eta: 0:01:52  lr: 0.000026  loss: 2.3622 (2.4765)  time: 0.3378  data: 0.0107  max mem: 1750\n",
            "Epoch: [3]  [420/781]  eta: 0:01:49  lr: 0.000026  loss: 2.4470 (2.4761)  time: 0.2790  data: 0.0093  max mem: 1750\n",
            "Epoch: [3]  [430/781]  eta: 0:01:46  lr: 0.000026  loss: 2.5797 (2.4771)  time: 0.2547  data: 0.0100  max mem: 1750\n",
            "Epoch: [3]  [440/781]  eta: 0:01:42  lr: 0.000026  loss: 2.5034 (2.4742)  time: 0.2563  data: 0.0105  max mem: 1750\n",
            "Epoch: [3]  [450/781]  eta: 0:01:40  lr: 0.000026  loss: 2.3219 (2.4698)  time: 0.3174  data: 0.0106  max mem: 1750\n",
            "Epoch: [3]  [460/781]  eta: 0:01:37  lr: 0.000026  loss: 2.3410 (2.4683)  time: 0.3425  data: 0.0101  max mem: 1750\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-353ba7f621a0>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m# 运行训练\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/deit/main.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    422\u001b[0m             \u001b[0mdata_loader_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msampler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         train_stats = train_one_epoch(\n\u001b[0m\u001b[1;32m    425\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_scaler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/deit/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, criterion, data_loader, optimizer, device, epoch, loss_scaler, max_norm, model_ema, mixup_fn, set_training_mode, args)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;31m# this attribute is added by timm on one optimizer (adahessian)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mis_second_order\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'is_second_order'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_second_order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         loss_scaler(loss, optimizer, clip_grad=max_norm,\n\u001b[0m\u001b[1;32m     68\u001b[0m                     parameters=model.parameters(), create_graph=is_second_order)\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/timm/utils/cuda.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, loss, optimizer, clip_grad, parameters, create_graph)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclip_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_scaler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mclip_grad\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}